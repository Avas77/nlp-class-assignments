{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "<img src=\"https://mma.prnewswire.com/media/1095203/East_Tennessee_State_University_Logo.jpg?p=facebook\" width=200 height=200 />\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1 style=\"text-align: center\">CSCI 5037 - NLP & Text Analysis</h1>\n",
    "</div>\n",
    "\n",
    "# <center>Text Cleaning </center>\n",
    "\n",
    "**<center>Dr. Ahmad Al-Doulat </center>**\n",
    "<center>Department of Computing </center>\n",
    "<center>East Tennessee State University</center>\n",
    "\n",
    "<hr style=\"border:2px solid lightblue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text cleaning is the process of preparing raw text for NLP (Natural Language Processing) so that machines can understand human language. This guide will underline text cleaning’s importance and go through some basic Python programming tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89515d5f9131c2e4b35d969ab43d372c19763db7"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "cats =>  cat\n",
      "trouble =>  troubl\n",
      "troubling => troubl\n",
      "troubled =>  troubl\n",
      "Lancaster Stemmer\n",
      "cats =>  cat\n",
      "trouble =>  troubl\n",
      "troubling => troubl\n",
      "troubled =>  troubl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "#provide a word to be stemmed\n",
    "print(\"Porter Stemmer\")\n",
    "print(\"cats => \",porter.stem(\"cats\"))\n",
    "print(\"trouble => \",porter.stem(\"trouble\"))\n",
    "print(\"troubling =>\", porter.stem(\"troubling\"))\n",
    "print(\"troubled => \",porter.stem(\"troubled\"))\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(\"cats => \",lancaster.stem(\"cats\"))\n",
    "print(\"trouble => \",lancaster.stem(\"trouble\"))\n",
    "print(\"troubling =>\",lancaster.stem(\"troubling\"))\n",
    "print(\"troubled => \",lancaster.stem(\"troubled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84b05c4bfe6394ee3ef2c319f3e125e1faf85536"
   },
   "source": [
    "**Stemming a  Complete Sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a909dc83f715a806ad551b0e7acea95097d92147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "x=stemSentence(sentence)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa6da7abea37807bc9a00a1bc8ae5034fe73b07b"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. \n",
    "\n",
    "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n",
    "\n",
    "Python NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\doula\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6952be9b9e3279adcf5bfc227bb9bc8aa0ff6cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ad2a8e9501483b3b1c004030b96b48fa837660e"
   },
   "source": [
    "In the above code output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. \n",
    "\n",
    "You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "399fe7f05121a2c91b766650c71808b90cf58943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with POS Tagging \n",
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatization with POS Tagging \")\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3678c45b4e1d35213f684304e3e716dc99aad1e"
   },
   "source": [
    "## Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "cf92283a81b0e540cc001699fd6c25068285fd6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1645), ('.', 556), ('and', 497), ('the', 439), ('to', 415), ('my', 375), ('of', 373), ('i', 361), ('that', 330), ('in', 329), ('thy', 268), ('thou', 235), (';', 231), (\"'s\", 206), ('love', 198), ('with', 187), ('is', 185), ('not', 180), ('for', 174), ('me', 170), ('a', 169), ('but', 165), ('thee', 162), ('be', 146), ('so', 145), ('as', 120), ('all', 117), ('you', 112), ('his', 109), ('which', 108), ('when', 106), ('this', 105), ('it', 104), ('by', 96), ('?', 96), ('your', 91), ('doth', 88), ('do', 86), ('from', 85), ('on', 81), ('or', 80), ('no', 80), ('have', 78), ('then', 74), ('what', 73), ('are', 71), ('beauty', 70), ('time', 69), ('if', 68), ('will', 64), ('more', 64), ('mine', 63), ('their', 63), ('shall', 60), ('heart', 57), ('eyes', 55), ('sweet', 55), ('can', 54), ('her', 53), ('they', 53), ('yet', 52), ('nor', 52), ('art', 51), ('o', 51), ('than', 48), ('now', 46), ('fair', 45), ('should', 44), ('thine', 44), ('he', 44), ('one', 44), ('make', 43), ('hath', 43), ('still', 42), ('how', 40), ('eye', 40), ('she', 38), ('him', 38), ('where', 37), ('true', 37), ('am', 36), ('see', 35), ('like', 35), (\"'\", 34), ('though', 34), ('world', 33), ('being', 33), ('those', 33), ('some', 32), (':', 31), ('such', 31), ('own', 30), ('were', 30), ('who', 30), ('live', 30), ('may', 30), ('say', 29), ('dost', 29), ('upon', 29), ('was', 29)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Read text file\n",
    "f = open(\"ShakespearesSonnets.txt\", \"r\")\n",
    "article = f.read()\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4886b23ba43fd7661d06235a00b30e80c0bc507"
   },
   "source": [
    "**Text Pre-Processing in Practice**\n",
    "\n",
    "Lets clean up text for better NLP results by removing stopwords, and lemmatizing. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4e13e93b1c95712d6f7c81e0c016eb67b006d6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thy', 268), ('thou', 235), ('love', 208), ('thee', 162), ('eye', 95), ('doth', 88), ('time', 78), ('beauty', 74), ('mine', 63), ('sweet', 61)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Read text file\n",
    "f = open(\"ShakespearesSonnets.txt\", \"r\")\n",
    "article = f.read()\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# English Stop words\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "Regular expressions are text matching patterns described with a formal syntax. You'll often hear regular expressions referred to as 'regex' or 'regexp' in conversation. Regular expressions can include a variety of rules, for finding repetition, to text-matching, and much more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Patterns in Text\n",
    "\n",
    "One of the most common uses for the re module is for finding patterns in text. Let's do a quick example of using the search method in the re module to find some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"term1\" in: \n",
      "\"This is a string with term1, but it does not have the other term.\"\n",
      "\n",
      "\n",
      "Match was found. \n",
      "\n",
      "Searching for \"term2\" in: \n",
      "\"This is a string with term1, but it does not have the other term.\"\n",
      "\n",
      "\n",
      "No Match was found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of patterns to search for\n",
    "patterns = [ 'term1', 'term2' ]\n",
    "\n",
    "# Text to parse\n",
    "text = 'This is a string with term1, but it does not have the other term.'\n",
    "\n",
    "for pattern in patterns:\n",
    "    print ('Searching for \"%s\" in: \\n\"%s\"' % (pattern, text)),\n",
    "    \n",
    "    #Check for match\n",
    "    if re.search(pattern,  text):\n",
    "        print ('\\n')\n",
    "        print ('Match was found. \\n')\n",
    "    else:\n",
    "        print ('\\n')\n",
    "        print ('No Match was found.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen that re.search() will take the pattern, scan the text, and then returns a **Match** object. If no pattern is found, a **None** is returned. To give a clearer picture of this match object, check out the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.Match"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of patterns to search for\n",
    "pattern = 'term1'\n",
    "\n",
    "# Text to parse\n",
    "text = 'This is a string with term1, but it does not have the other term.'\n",
    "\n",
    "match = re.search(pattern,  text)\n",
    "\n",
    "type(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **Match** object returned by the search() method is more than just a Boolean or None, it contains information about the match, including the original input string, the regular expression that was used, and the location of the match. Let's see the methods we can use on the match object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show start of match\n",
    "match.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show end\n",
    "match.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split with regular expressions\n",
    "\n",
    "Let's see how we can split with the re syntax. This should look similar to how you used the split() method with strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the domain name of someone with the email: hello', 'gmail.com']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term to split on\n",
    "split_term = '@'\n",
    "\n",
    "phrase = 'What is the domain name of someone with the email: hello@gmail.com'\n",
    "\n",
    "# Split the phrase\n",
    "re.split(split_term,phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how re.split() returns a list with the term to spit on removed and the terms in the list are a split up version of the string. Create a couple of more examples for yourself to make sure you understand!\n",
    "\n",
    "## Finding all instances of a pattern\n",
    "\n",
    "You can use re.findall() to find all the instances of a pattern in a string. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['match']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of all matches\n",
    "re.findall('match','test phrase match is in middle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern re Syntax\n",
    "\n",
    "Regular expressions supports a huge variety of patterns the just simply finding where a single string occurred. \n",
    "\n",
    "We can use *metacharacters* along with re to find specific types of patterns. \n",
    "\n",
    "Since we will be testing multiple re syntax forms, let's create a function that will print out results given a list of various regular expressions and a phrase to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_re_find(patterns,phrase):\n",
    "    '''\n",
    "    Takes in a list of regex patterns\n",
    "    Prints a list of all matches\n",
    "    '''\n",
    "    for pattern in patterns:\n",
    "        print( 'Searching the phrase using the re check: %r' %pattern)\n",
    "        print( re.findall(pattern,phrase))\n",
    "        print( '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetition Syntax\n",
    "\n",
    "There are five ways to express repetition in a pattern:\n",
    "\n",
    "1. A pattern followed by the meta-character * is repeated zero or more times. \n",
    "2. Replace the * with + and the pattern must appear at least once. \n",
    "3. Using ? means the pattern appears zero or one time. \n",
    "4. For a specific number of occurrences, use {m} after the pattern, where m is replaced with the number of times         the pattern should repeat. \n",
    "5. Use {m,n} where m is the minimum number of repetitions and n is the maximum. Leaving out n ({m,}) means the           value appears at least m times, with no maximum.\n",
    "    \n",
    "Now we will see an example of each of these using our multi_re_find function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: 'sd*'\n",
      "['sd', 'sd', 's', 's', 'sddd', 'sddd', 'sddd', 'sd', 's', 's', 's', 's', 's', 's', 'sdddd']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: 'sd+'\n",
      "['sd', 'sd', 'sddd', 'sddd', 'sddd', 'sd', 'sdddd']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: 'sd?'\n",
      "['sd', 'sd', 's', 's', 'sd', 'sd', 'sd', 'sd', 's', 's', 's', 's', 's', 's', 'sd']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: 'sd{3}'\n",
      "['sddd', 'sddd', 'sddd', 'sddd']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: 'sd{2,3}'\n",
      "['sddd', 'sddd', 'sddd', 'sddd']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'sdsd..sssddd...sdddsddd...dsds...dsssss...sdddd'\n",
    "\n",
    "test_patterns = [ 'sd*',     # s followed by zero or more d's\n",
    "                'sd+',          # s followed by one or more d's\n",
    "                'sd?',          # s followed by zero or one d's\n",
    "                'sd{3}',        # s followed by three d's\n",
    "                'sd{2,3}',      # s followed by two to three d's\n",
    "                ]\n",
    "\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Sets\n",
    "\n",
    "Character sets are used when you wish to match any one of a group of characters at a point in the input. Brackets are used to construct character set inputs. For example: the input [ab] searches for occurrences of either a or b.\n",
    "Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: '[sd]'\n",
      "['s', 'd', 's', 'd', 's', 's', 's', 'd', 'd', 'd', 's', 'd', 'd', 'd', 's', 'd', 'd', 'd', 'd', 's', 'd', 's', 'd', 's', 's', 's', 's', 's', 's', 'd', 'd', 'd', 'd']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: 's[sd]+'\n",
      "['sdsd', 'sssddd', 'sdddsddd', 'sds', 'sssss', 'sdddd']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'sdsd..sssddd...sdddsddd...dsds...dsssss...sdddd'\n",
    "\n",
    "test_patterns = [ '[sd]',    # either s or d\n",
    "            's[sd]+']   # s followed by one or more s or d\n",
    "            \n",
    "\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that the first \\[sd\\] returns every instance. Also the second input will just return any thing starting with an s in this particular case of the test phrase input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusion\n",
    "\n",
    "We can use ^ to exclude terms by incorporating it into the bracket syntax notation. For example: [^...] will match any single character not in the brackets. Let's see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'This is a string! But it has punctuation. How can we remove it?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [^!.? ] to check for matches that are not a !,.,?, or space. Add the + to check that the match appears at least once, this basically translate into finding the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'string',\n",
       " 'But',\n",
       " 'it',\n",
       " 'has',\n",
       " 'punctuation',\n",
       " 'How',\n",
       " 'can',\n",
       " 'we',\n",
       " 'remove',\n",
       " 'it']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^!.? ]+',test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Ranges\n",
    "\n",
    "As character sets grow larger, typing every character that should (or should not) match could become very tedious. A more compact format using character ranges lets you define a character set to include all of the contiguous characters between a start and stop point. The format used is [start-end].\n",
    "\n",
    "Common use cases are to search for a specific range of letters in the alphabet, such [a-f] would return matches with any instance of letters between a and f. \n",
    "\n",
    "Let's walk through some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: '[a-z]+'\n",
      "['his', 'is', 'an', 'example', 'sentence', 'ets', 'see', 'if', 'we', 'can', 'find', 'some', 'letters']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '[A-Z]+'\n",
      "['T', 'L']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '[a-zA-Z]+'\n",
      "['This', 'is', 'an', 'example', 'sentence', 'Lets', 'see', 'if', 'we', 'can', 'find', 'some', 'letters']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '[A-Z][a-z]+'\n",
      "['This', 'Lets']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_phrase = 'This is an example sentence. Lets see if we can find some letters.'\n",
    "\n",
    "test_patterns=[ '[a-z]+',      # sequences of lower case letters\n",
    "                '[A-Z]+',      # sequences of upper case letters\n",
    "                '[a-zA-Z]+',   # sequences of lower or upper case letters\n",
    "                '[A-Z][a-z]+'] # one upper case letter followed by lower case letters\n",
    "                \n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape Codes\n",
    "\n",
    "You can use special escape codes to find specific types of patterns in your data, such as digits, non-digits,whitespace, and more. For example:\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"14%\" />\n",
    "<col width=\"86%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Code</th>\n",
    "<th class=\"head\">Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\d</span></tt></td>\n",
    "<td>a digit</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\D</span></tt></td>\n",
    "<td>a non-digit</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\s</span></tt></td>\n",
    "<td>whitespace (tab, space, newline, etc.)</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\S</span></tt></td>\n",
    "<td>non-whitespace</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\w</span></tt></td>\n",
    "<td>alphanumeric</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><tt class=\"docutils literal\"><span class=\"pre\">\\W</span></tt></td>\n",
    "<td>non-alphanumeric</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Escapes are indicated by prefixing the character with a backslash (\\). Unfortunately, a backslash must itself be escaped in normal Python strings, and that results in expressions that are difficult to read. Using raw strings, created by prefixing the literal value with r, for creating regular expressions eliminates this problem and maintains readability.\n",
    "\n",
    "The use of r to escape a backslash is probably one of the things that block someone who is not familiar with regex in Python from being able to read regex code at first. Hopefully after seeing these examples this syntax will become clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the phrase using the re check: '\\\\d+'\n",
      "['1233']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '\\\\D+'\n",
      "['This is a string with some numbers ', ' and a symbol #hashtag']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '\\\\s+'\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '\\\\S+'\n",
      "['This', 'is', 'a', 'string', 'with', 'some', 'numbers', '1233', 'and', 'a', 'symbol', '#hashtag']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '\\\\w+'\n",
      "['This', 'is', 'a', 'string', 'with', 'some', 'numbers', '1233', 'and', 'a', 'symbol', 'hashtag']\n",
      "\n",
      "\n",
      "Searching the phrase using the re check: '\\\\W+'\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' #']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'This is a string with some numbers 1233 and a symbol #hashtag'\n",
    "\n",
    "test_patterns=[ r'\\d+', # sequence of digits\n",
    "                r'\\D+', # sequence of non-digits\n",
    "                r'\\s+', # sequence of whitespace\n",
    "                r'\\S+', # sequence of non-whitespace\n",
    "                r'\\w+', # alphanumeric characters\n",
    "                r'\\W+', # non-alphanumeric\n",
    "                ]\n",
    "\n",
    "multi_re_find(test_patterns,test_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Resources\n",
    "For more on regular expressions, check out the [documentation](https://docs.python.org/2/library/re.html#regular-expression-syntax).\n",
    "\n",
    "You can also check out the nice summary tables at this [source](http://www.tutorialspoint.com/python/python_reg_expressions.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
