{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25ff9a3-e4e5-41c7-a78e-2236fa495633",
   "metadata": {},
   "source": [
    "# Text Cleaning in NLP - Hands-On Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "758c2c01-b2be-4757-87fa-eeb149769e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad1af2a8-7b29-475f-8f96-5cf96b470bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c46ae27-11a8-43ae-824e-ab9dc3b0037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text data\n",
    "text_data = [\n",
    "    \"Hello! I looooveee this movie!!! üòçüòçüòç #awesome http://movie.com\",\n",
    "    \"Check out my blog at http://example.com #blog\",\n",
    "    \"This is an example sentence.\",\n",
    "    \"What a great day, @John! Let's grab lunch :)\",\n",
    "    \"I can't believe it's already 2024!\",\n",
    "    \"Running, ran, runner!\",\n",
    "    np.nan  # Added NaN for demonstration purposes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8f4f3f6-f126-434d-b135-66350582cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Preprocessing\n",
    "def basic_preprocessing(text):\n",
    "    # Check if the text is a string, else return empty string or handle NaN\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e746ba-1a39-47bb-a856-58ff552d4906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Basic Preprocessing =====\n",
      "Text 1: hello! i looooveee this movie!!! üòçüòçüòç #awesome http://movie.com\n",
      "Text 2: check out my blog at http://example.com #blog\n",
      "Text 3: this is an example sentence.\n",
      "Text 4: what a great day, @john! let's grab lunch :)\n",
      "Text 5: i can't believe it's already 2024!\n",
      "Text 6: running, ran, runner!\n",
      "Text 7: \n"
     ]
    }
   ],
   "source": [
    "# Apply to text data\n",
    "cleaned_data = [basic_preprocessing(text) for text in text_data]\n",
    "print(\"\\n===== Basic Preprocessing =====\")\n",
    "for i, text in enumerate(cleaned_data):\n",
    "    print(f\"Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06a7d590-087e-40de-9e2d-dbbb0f62e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenization\n",
    "def tokenize_text(text):\n",
    "    # Sentence Tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Word Tokenization for each sentence\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdd6d289-68be-415c-b134-a029a14b706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Tokenization =====\n",
      "Sentence 1 Tokens: ['hello', '!']\n",
      "Sentence 2 Tokens: ['i', 'looooveee', 'this', 'movie', '!', '!', '!']\n",
      "Sentence 3 Tokens: ['üòçüòçüòç', '#', 'awesome', 'http', ':', '//movie.com']\n"
     ]
    }
   ],
   "source": [
    "# Apply to one example\n",
    "example_text = cleaned_data[0]\n",
    "tokens = tokenize_text(example_text)\n",
    "print(\"\\n===== Tokenization =====\")\n",
    "for i, sentence_tokens in enumerate(tokens):\n",
    "    print(f\"Sentence {i+1} Tokens: {sentence_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "215acbb8-e2d4-47fd-9a8c-c811030c20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Handling Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a5c3de3-cba6-4ab9-898b-517e30d0b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Tokens without Stopwords =====\n",
      "Sentence 1 Tokens: ['hello', '!']\n",
      "Sentence 2 Tokens: ['looooveee', 'movie', '!', '!', '!']\n",
      "Sentence 3 Tokens: ['üòçüòçüòç', '#', 'awesome', 'http', ':', '//movie.com']\n"
     ]
    }
   ],
   "source": [
    "# Apply to one example\n",
    "tokens_without_stopwords = [remove_stopwords(sentence) for sentence in tokens]\n",
    "print(\"\\n===== Tokens without Stopwords =====\")\n",
    "for i, sentence_tokens in enumerate(tokens_without_stopwords):\n",
    "    print(f\"Sentence {i+1} Tokens: {sentence_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f062ef78-a00b-448f-8b22-0a6132d57cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dealing with Noise\n",
    "def remove_noise(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9215796-1d46-47ed-b874-adb510c48527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Noise Removed Text =====\n",
      "Text 1: hello i looooveee this movie  awesome \n",
      "Text 2: check out my blog at  blog\n",
      "Text 3: this is an example sentence\n",
      "Text 4: what a great day john lets grab lunch \n",
      "Text 5: i cant believe its already 2024\n",
      "Text 6: running ran runner\n",
      "Text 7: \n"
     ]
    }
   ],
   "source": [
    "# Apply to text data\n",
    "noise_removed_data = [remove_noise(text) for text in cleaned_data]\n",
    "print(\"\\n===== Noise Removed Text =====\")\n",
    "for i, text in enumerate(noise_removed_data):\n",
    "    print(f\"Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96e4f08c-8d10-409a-9139-6dc849ff389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Normalization Techniques (Stemming and Lemmatization)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    # Stemming\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return stemmed, lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2db92f41-832a-4452-926e-49326099be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Stemming and Lemmatization =====\n",
      "Stemmed: ['hello', 'i', 'loooovee', 'thi', 'movi', 'awesom']\n",
      "Lemmatized: ['hello', 'i', 'looooveee', 'this', 'movie', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# Apply to one example\n",
    "tokens = word_tokenize(noise_removed_data[0])\n",
    "stemmed, lemmatized = normalize_text(tokens)\n",
    "print(\"\\n===== Stemming and Lemmatization =====\")\n",
    "print(f\"Stemmed: {stemmed}\")\n",
    "print(f\"Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56e83516-79f8-409b-bfeb-23fe6be1fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Regular Expressions (RegEx)\n",
    "def apply_regex(text):\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Extract emails (example pattern, although the data doesn't have emails)\n",
    "    emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    \n",
    "    return text, emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe2e6b11-ed2b-41c7-a6b9-62d54dba2736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RegEx (After Removing Digits) =====\n",
      "Text: hello i looooveee this movie  awesome \n",
      "Extracted Emails: None\n"
     ]
    }
   ],
   "source": [
    "# Apply to one example\n",
    "regex_applied, extracted_emails = apply_regex(noise_removed_data[0])\n",
    "print(\"\\n===== RegEx (After Removing Digits) =====\")\n",
    "print(f\"Text: {regex_applied}\")\n",
    "print(f\"Extracted Emails: {extracted_emails if extracted_emails else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18670e26-4e4b-44a7-9774-8d76c5347f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Cleaned DataFrame (After Removing Duplicates and NaNs) =====\n",
      "                    text\n",
      "0    This is a sentence.\n",
      "2      Another sentence.\n",
      "4  Running, ran, runner!\n"
     ]
    }
   ],
   "source": [
    "# 7. Dealing with Duplicates and Incomplete Text\n",
    "# Example text data with duplicates and missing values\n",
    "text_data = [\n",
    "    \"This is a sentence.\",\n",
    "    \"This is a sentence.\",\n",
    "    \"Another sentence.\",\n",
    "    np.nan,\n",
    "    \"Running, ran, runner!\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(text_data, columns=[\"text\"])\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Handle missing values\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "print(\"\\n===== Cleaned DataFrame (After Removing Duplicates and NaNs) =====\")\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdc26096-acc5-4fce-808b-e5da5c847d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Advanced Cleaning Techniques (Handling Slang)\n",
    "slang_dict = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"brb\": \"be right back\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17d415f4-5701-48bf-90a8-7463935981f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slang(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([slang_dict[word] if word in slang_dict else word for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abd3da56-a6f1-47ef-b7b4-c4cc13478930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== After Replacing Slang =====\n",
      "Text: hello i looooveee this movie awesome\n"
     ]
    }
   ],
   "source": [
    "# Apply to one example\n",
    "advanced_cleaned_text = replace_slang(noise_removed_data[0])\n",
    "print(\"\\n===== After Replacing Slang =====\")\n",
    "print(f\"Text: {advanced_cleaned_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af15316e-9d76-4157-866b-b60b3adc8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Putting It All Together: Building a Cleaning Pipeline\n",
    "def text_cleaning_pipeline(text):\n",
    "    # Basic Preprocessing\n",
    "    text = basic_preprocessing(text)\n",
    "    \n",
    "    # Remove noise\n",
    "    text = remove_noise(text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Normalize (Stemming or Lemmatization)\n",
    "    _, tokens = normalize_text(tokens)\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9fb4ece-1950-4a42-ae5a-0a1fdd1ebc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Final Cleaned Data =====\n",
      "Text 1: sentence\n",
      "Text 2: sentence\n",
      "Text 3: another sentence\n",
      "Text 4: \n",
      "Text 5: running ran runner\n"
     ]
    }
   ],
   "source": [
    "# Apply to the entire dataset\n",
    "final_cleaned_data = [text_cleaning_pipeline(text) for text in text_data]\n",
    "print(\"\\n===== Final Cleaned Data =====\")\n",
    "for i, text in enumerate(final_cleaned_data):\n",
    "    print(f\"Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1784011-cfcc-410d-84bc-8b515194833e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
